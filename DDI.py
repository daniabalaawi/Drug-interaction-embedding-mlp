# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gU7N6WD5-dmexZ5b5WP9p31zLUWmE_uF
"""

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load a manageable sample of the data
data = pd.read_csv('TWOSIDES.csv')  # Increase if Colab can handle it

# 2. Keep only the top 50 most frequent conditions
top_conditions = data['condition_concept_name'].value_counts().nlargest(30).index
data = data[data['condition_concept_name'].isin(top_conditions)]

# 3. Downcast numeric columns to reduce memory use
numeric_cols = ['A', 'B', 'C', 'D', 'PRR', 'PRR_error', 'mean_reporting_frequency']
for col in numeric_cols:
    data[col] = pd.to_numeric(data[col], downcast='float')

# 4. Encode categorical variables to numbers
le_drug1 = LabelEncoder()
le_drug2 = LabelEncoder()
le_condition = LabelEncoder()

data['drug_1_enc'] = le_drug1.fit_transform(data['drug_1_concept_name'])
data['drug_2_enc'] = le_drug2.fit_transform(data['drug_2_concept_name'])
data['condition_enc'] = le_condition.fit_transform(data['condition_concept_name'])

# 5. Define input features and target label
features = ['drug_1_enc', 'drug_2_enc'] + numeric_cols
X = data[features]
y = data['condition_enc']

# 6. Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 7. Train the Logistic Regression model
model = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')
model.fit(X_train, y_train)

# 8. Make predictions on the test set
y_pred = model.predict(X_test)

# 9. Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
#print(classification_report(y_test, y_pred, target_names=le_condition.classes_))

# 10. Plot confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(12, 8))
sns.heatmap(conf_matrix, annot=False, fmt='d', cmap='Blues',
            xticklabels=le_condition.classes_,
            yticklabels=le_condition.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (Top 50 Conditions)")
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load a manageable sample of the data
data = pd.read_csv('TWOSIDES.csv')  # You can increase nrows if Colab can handle it

# 2. Keep only the top 50 most frequent conditions
top_conditions = data['condition_concept_name'].value_counts().nlargest(50).index
data = data[data['condition_concept_name'].isin(top_conditions)]

# 3. Downcast numeric columns to save memory
numeric_cols = ['A', 'B', 'C', 'D', 'PRR', 'PRR_error', 'mean_reporting_frequency']
for col in numeric_cols:
    data[col] = pd.to_numeric(data[col], downcast='float')

# 4. Encode categorical variables
le_drug1 = LabelEncoder()
le_drug2 = LabelEncoder()
le_condition = LabelEncoder()

data['drug_1_enc'] = le_drug1.fit_transform(data['drug_1_concept_name'])
data['drug_2_enc'] = le_drug2.fit_transform(data['drug_2_concept_name'])
data['condition_enc'] = le_condition.fit_transform(data['condition_concept_name'])

# 5. Define features and target
features = ['drug_1_enc', 'drug_2_enc'] + numeric_cols
X = data[features]
y = data['condition_enc']

# 6. Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 7. Train a Random Forest Classifier
model = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

# 8. Predict on test data
y_pred = model.predict(X_test)

# 9. Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=le_condition.classes_))

# 10. Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(12, 8))
sns.heatmap(conf_matrix, annot=False, fmt='d', cmap='Blues',
            xticklabels=le_condition.classes_,
            yticklabels=le_condition.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (Random Forest, Top 50 Conditions)")
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load data
data = pd.read_csv('TWOSIDES.csv')

# 2. Keep only top 20 most frequent conditions
top_conditions = data['condition_concept_name'].value_counts().nlargest(20).index
data = data[data['condition_concept_name'].isin(top_conditions)]

# 3. Downcast numeric columns to save memory
numeric_cols = ['A', 'B', 'C', 'D', 'PRR', 'PRR_error', 'mean_reporting_frequency']
for col in numeric_cols:
    data[col] = pd.to_numeric(data[col], downcast='float')

# 4. Create a single encoded feature for drug pair
data['drug_pair'] = data['drug_1_concept_name'] + ' + ' + data['drug_2_concept_name']
le_pair = LabelEncoder()
data['drug_pair_enc'] = le_pair.fit_transform(data['drug_pair'])

# 5. Encode condition
le_condition = LabelEncoder()
data['condition_enc'] = le_condition.fit_transform(data['condition_concept_name'])

# 6. Define features and target
features = ['drug_pair_enc'] + numeric_cols
X = data[features]
y = data['condition_enc']

# 7. Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 8. Train the model
model = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

# 9. Predict and evaluate
y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=le_condition.classes_))

# 10. Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(12, 8))
sns.heatmap(conf_matrix, annot=False, fmt='d', cmap='Blues',
            xticklabels=le_condition.classes_,
            yticklabels=le_condition.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (Top 20 Conditions + Drug Pair)")
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from xgboost import XGBClassifier

# 1. Load a subset of your dataset
data = pd.read_csv('TWOSIDES.csv')

# 2. Keep only the top 20 most frequent conditions
top_conditions = data['condition_concept_name'].value_counts().nlargest(20).index
data = data[data['condition_concept_name'].isin(top_conditions)]

# 3. Downcast numeric columns
numeric_cols = ['A', 'B', 'C', 'D', 'PRR', 'PRR_error', 'mean_reporting_frequency']
for col in numeric_cols:
    data[col] = pd.to_numeric(data[col], downcast='float')

# 4. Encode drug pair
data['drug_pair'] = data['drug_1_concept_name'] + ' + ' + data['drug_2_concept_name']
data['drug_pair_enc'] = LabelEncoder().fit_transform(data['drug_pair'])

# 5. Encode condition (target)
data['condition_enc'] = LabelEncoder().fit_transform(data['condition_concept_name'])

# 6. Define features and target
features = ['drug_pair_enc'] + numeric_cols
X = data[features]
y = data['condition_enc']

# 7. Split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 8. Train XGBoost model
model = XGBClassifier(n_estimators=200,
                      max_depth=10,
                      learning_rate=0.1,
                      objective='multi:softmax',
                      num_class=len(set(y)),
                      eval_metric='mlogloss',
                      use_label_encoder=False,
                      random_state=42)

model.fit(X_train, y_train)

# 9. Predict and evaluate
y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# 10. Plot confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(12, 8))
sns.heatmap(conf_matrix, annot=False, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (XGBoost, Top 20 Conditions)")
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from xgboost import XGBClassifier

# 1. Load a subset of your dataset
data = pd.read_csv('TWOSIDES.csv')

# 2. Keep only the top 20 most frequent conditions
top_conditions = data['condition_concept_name'].value_counts().nlargest(20).index
data = data[data['condition_concept_name'].isin(top_conditions)]

# 3. Downcast numeric columns
numeric_cols = ['A', 'B', 'C', 'D', 'PRR', 'PRR_error', 'mean_reporting_frequency']
for col in numeric_cols:
    data[col] = pd.to_numeric(data[col], downcast='float')

# 4. Encode drug pair
data['drug_pair'] = data['drug_1_concept_name'] + ' + ' + data['drug_2_concept_name']
data['drug_pair_enc'] = LabelEncoder().fit_transform(data['drug_pair'])

# 5. Encode condition (target)
data['condition_enc'] = LabelEncoder().fit_transform(data['condition_concept_name'])

# 6. Define features and target
features = ['drug_pair_enc'] + numeric_cols
X = data[features]
y = data['condition_enc']

# 7. Split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 8. Define parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# 9. Create a GridSearchCV object
grid_search = GridSearchCV(
    estimator=XGBClassifier(use_label_encoder=False, objective='multi:softmax'),
    param_grid=param_grid,
    cv=3,  # 3-fold cross-validation
    n_jobs=-1,  # Use all cores
    verbose=1,  # Show progress
    scoring='accuracy'
)

# 10. Fit the grid search to find the best parameters
grid_search.fit(X_train, y_train)

# 11. Print the best parameters
print("Best parameters:", grid_search.best_params_)

# 12. Use the best model to predict on the test set
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# 13. Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# 14. Plot confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(12, 8))
sns.heatmap(conf_matrix, annot=False, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (XGBoost, Top 20 Conditions)")
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical

# 1. Load and preprocess
data = pd.read_csv('TWOSIDES.csv')

# 2. Filter top 20 conditions
top_conditions = data['condition_concept_name'].value_counts().nlargest(20).index
data = data[data['condition_concept_name'].isin(top_conditions)]

# 3. Downcast numeric features
numeric_cols = ['A', 'B', 'C', 'D', 'PRR', 'PRR_error', 'mean_reporting_frequency']
for col in numeric_cols:
    data[col] = pd.to_numeric(data[col], downcast='float')

# 4. Encode drugs separately
data['drug_1_enc'] = LabelEncoder().fit_transform(data['drug_1_concept_name'])
data['drug_2_enc'] = LabelEncoder().fit_transform(data['drug_2_concept_name'])

# 5. Encode target condition
data['condition_enc'] = LabelEncoder().fit_transform(data['condition_concept_name'])
num_classes = data['condition_enc'].nunique()

# 6. Prepare features and labels
features = ['drug_1_enc', 'drug_2_enc'] + numeric_cols
X = data[features].values
y = data['condition_enc'].values
y_cat = to_categorical(y, num_classes=num_classes)

# 7. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)

# 8. Reshape for CNN: (samples, features, 1)
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# 9. Build CNN model
model = Sequential([
    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Conv1D(128, kernel_size=3, activation='relu'),
    BatchNormalization(),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.4),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# 10. Train
model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

# 11. Evaluate
y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)
y_true = np.argmax(y_test, axis=1)

print("Accuracy:", accuracy_score(y_true, y_pred))
print(classification_report(y_true, y_pred))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1. Load dataset
data = pd.read_csv('TWOSIDES.csv')

# 2. Keep only top 20 most frequent conditions
top_conditions = data['condition_concept_name'].value_counts().nlargest(20).index
data = data[data['condition_concept_name'].isin(top_conditions)]

# 3. Downcast numeric columns
numeric_cols = ['A', 'B', 'C', 'D', 'PRR', 'PRR_error', 'mean_reporting_frequency']
for col in numeric_cols:
    data[col] = pd.to_numeric(data[col], downcast='float')

# 4. Encode drug names separately
drug1_enc = LabelEncoder()
drug2_enc = LabelEncoder()
data['drug_1_enc'] = drug1_enc.fit_transform(data['drug_1_concept_name'])
data['drug_2_enc'] = drug2_enc.fit_transform(data['drug_2_concept_name'])

# 5. Encode target variable
label_enc = LabelEncoder()
data['condition_enc'] = label_enc.fit_transform(data['condition_concept_name'])

# 6. Feature & target definition
features = ['drug_1_enc', 'drug_2_enc'] + numeric_cols
X = data[features]
y = data['condition_enc']

# 7. Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 8. Define base models
xgb = XGBClassifier(n_estimators=150, max_depth=8, learning_rate=0.1,
                    objective='multi:softprob', num_class=len(set(y)),
                    eval_metric='mlogloss', use_label_encoder=False, random_state=42)

rf = RandomForestClassifier(n_estimators=200, max_depth=12, random_state=42)

lr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')

# 9. Voting Classifier (Soft Voting)
voting_clf = VotingClassifier(
    estimators=[('xgb', xgb), ('rf', rf), ('lr', lr)],
    voting='soft'
)

# 10. Fit and predict
voting_clf.fit(X_train, y_train)
y_pred = voting_clf.predict(X_test)

# 11. Evaluate
print("Ensemble Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=label_enc.classes_))

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, top_k_accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load data
data = pd.read_csv('TWOSIDES.csv')

# 2. Keep only top 20 most frequent conditions
top_conditions = data['condition_concept_name'].value_counts().nlargest(5).index
data = data[data['condition_concept_name'].isin(top_conditions)]

# 3. Downcast numeric columns
numeric_cols = ['A', 'B', 'C', 'D', 'PRR', 'PRR_error', 'mean_reporting_frequency']
for col in numeric_cols:
    data[col] = pd.to_numeric(data[col], downcast='float')

# 4. Encode drugs separately
le_drug1 = LabelEncoder()
le_drug2 = LabelEncoder()
data['drug_1_enc'] = le_drug1.fit_transform(data['drug_1_concept_name'])
data['drug_2_enc'] = le_drug2.fit_transform(data['drug_2_concept_name'])

# 5. Encode condition (target)
le_condition = LabelEncoder()
data['condition_enc'] = le_condition.fit_transform(data['condition_concept_name'])

# 6. Feature Engineering (Optional but recommended)
data['interaction_score'] = data['PRR'] * data['mean_reporting_frequency']
data['error_ratio'] = data['PRR_error'] / (data['PRR'] + 1e-5)

# 7. Define features and target
features = ['drug_1_enc', 'drug_2_enc'] + numeric_cols + ['interaction_score', 'error_ratio']
X = data[features]
y = data['condition_enc']

# 8. Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 9. Train model
model = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

# 10. Predict and evaluate
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=le_condition.classes_))

# 11. Optional: Top-k Accuracy (e.g., top-3)
y_prob = model.predict_proba(X_test)
top3_acc = top_k_accuracy_score(y_test, y_prob, k=3)
print("Top-3 Accuracy:", top3_acc)

# 12. Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(12, 8))
sns.heatmap(conf_matrix, annot=False, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (Random Forest, Top 20 Conditions)")
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, top_k_accuracy_score
from catboost import CatBoostClassifier, Pool
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load data
data = pd.read_csv('TWOSIDES.csv')

# 2. Keep only top 5 most frequent conditions
top_conditions = data['condition_concept_name'].value_counts().nlargest(5).index
data = data[data['condition_concept_name'].isin(top_conditions)]

# 3. Downcast numeric columns
numeric_cols = ['A', 'B', 'C', 'D', 'PRR', 'PRR_error', 'mean_reporting_frequency']
for col in numeric_cols:
    data[col] = pd.to_numeric(data[col], downcast='float')

# 4. Encode drugs as categorical strings (CatBoost handles this natively)
data['drug_1'] = data['drug_1_concept_name'].astype(str)
data['drug_2'] = data['drug_2_concept_name'].astype(str)

# 5. Encode condition
le_condition = LabelEncoder()
data['condition_enc'] = le_condition.fit_transform(data['condition_concept_name'])

# 6. Feature Engineering
data['interaction_score'] = data['PRR'] * data['mean_reporting_frequency']
data['error_ratio'] = data['PRR_error'] / (data['PRR'] + 1e-5)

# 7. Define features and categorical features
features = ['drug_1', 'drug_2'] + numeric_cols + ['interaction_score', 'error_ratio']
cat_features = ['drug_1', 'drug_2']
X = data[features]
y = data['condition_enc']

# 8. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 9. Create Pool objects for CatBoost
train_pool = Pool(X_train, y_train, cat_features=cat_features)
test_pool = Pool(X_test, y_test, cat_features=cat_features)

# 10. Train CatBoost model
model = CatBoostClassifier(
    iterations=300,
    learning_rate=0.1,
    depth=8,
    loss_function='MultiClass',
    eval_metric='Accuracy',
    random_seed=42,
    verbose=100
)

model.fit(train_pool)

# 11. Predict and Evaluate
y_pred = model.predict(test_pool).flatten()
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=le_condition.classes_))

# 12. Top-k Accuracy
y_probs = model.predict_proba(test_pool)
print("Top-3 Accuracy:", top_k_accuracy_score(y_test, y_probs, k=3))

!pip install catboost

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, top_k_accuracy_score
from sklearn.neighbors import KNeighborsClassifier

# 1. Load data
data = pd.read_csv('TWOSIDES.csv')

# 2. Keep only top 5 most frequent conditions
top_conditions = data['condition_concept_name'].value_counts().nlargest(5).index
data = data[data['condition_concept_name'].isin(top_conditions)]

# 3. Downcast numeric columns
numeric_cols = ['A', 'B', 'C', 'D', 'PRR', 'PRR_error', 'mean_reporting_frequency']
for col in numeric_cols:
    data[col] = pd.to_numeric(data[col], downcast='float')

# 4. Encode drug names using LabelEncoder
le_drug1 = LabelEncoder()
le_drug2 = LabelEncoder()
data['drug_1_enc'] = le_drug1.fit_transform(data['drug_1_concept_name'].astype(str))
data['drug_2_enc'] = le_drug2.fit_transform(data['drug_2_concept_name'].astype(str))

# 5. Encode target
le_condition = LabelEncoder()
data['condition_enc'] = le_condition.fit_transform(data['condition_concept_name'])

# 6. Feature Engineering
data['interaction_score'] = data['PRR'] * data['mean_reporting_frequency']
data['error_ratio'] = data['PRR_error'] / (data['PRR'] + 1e-5)

# 7. Define features and target
features = ['drug_1_enc', 'drug_2_enc'] + numeric_cols + ['interaction_score', 'error_ratio']
X = data[features]
y = data['condition_enc']

# 8. Feature scaling (important for KNN)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 9. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# 10. Train KNN Model
knn = KNeighborsClassifier(n_neighbors=5, weights='distance')  # Try different n_neighbors
knn.fit(X_train, y_train)

# 11. Predict and Evaluate
y_pred = knn.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=le_condition.classes_))

# 12. Top-3 Accuracy
y_probs = knn.predict_proba(X_test)
print("Top-3 Accuracy:", top_k_accuracy_score(y_test, y_probs, k=3))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load your dataset
df = pd.read_csv('TWOSIDES.csv')  # Update with your actual filename

# Encode categorical ID columns
label_encoders = {}
for col in ['drug_1_rxnorn_id', 'drug_2_rxnorm_id', 'condition_meddra_id']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

# Define features and label
X = df[['drug_1_rxnorn_id', 'drug_2_rxnorm_id', 'condition_meddra_id', 'A', 'B', 'C', 'D', 'PRR_error', 'mean_reporting_frequency']]
y = (df['PRR'] > 1.0).astype(int)  # Binary label

# Normalize numerical features
scaler = StandardScaler()
X[['A', 'B', 'C', 'D', 'PRR_error', 'mean_reporting_frequency']] = scaler.fit_transform(
    X[['A', 'B', 'C', 'D', 'PRR_error', 'mean_reporting_frequency']]
)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Define vocabulary sizes
vocab_size = {
    'drug_1': df['drug_1_rxnorn_id'].nunique(),
    'drug_2': df['drug_2_rxnorm_id'].nunique(),
    'condition': df['condition_meddra_id'].nunique()
}

# Inputs
input_drug1 = Input(shape=(1,), name='drug_1')
input_drug2 = Input(shape=(1,), name='drug_2')
input_condition = Input(shape=(1,), name='condition')
input_numeric = Input(shape=(6,), name='numeric')  # A, B, C, D, PRR_error, mean_reporting_frequency

# Embeddings
embed_drug1 = Embedding(input_dim=vocab_size['drug_1'] + 1, output_dim=8)(input_drug1)
embed_drug2 = Embedding(input_dim=vocab_size['drug_2'] + 1, output_dim=8)(input_drug2)
embed_condition = Embedding(input_dim=vocab_size['condition'] + 1, output_dim=8)(input_condition)

# Flatten
flat_drug1 = Flatten()(embed_drug1)
flat_drug2 = Flatten()(embed_drug2)
flat_condition = Flatten()(embed_condition)

# Concatenate all inputs
concat = Concatenate()([flat_drug1, flat_drug2, flat_condition, input_numeric])

# Dense Layers
x = Dense(64, activation='relu')(concat)
x = Dense(32, activation='relu')(x)
output = Dense(1, activation='sigmoid')(x)

# Compile the model
model = Model(inputs=[input_drug1, input_drug2, input_condition, input_numeric], outputs=output)
model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

model.fit(
    x={
        'drug_1': X_train['drug_1_rxnorn_id'],
        'drug_2': X_train['drug_2_rxnorm_id'],
        'condition': X_train['condition_meddra_id'],
        'numeric': X_train[['A', 'B', 'C', 'D', 'PRR_error', 'mean_reporting_frequency']]
    },
    y=y_train,
    validation_split=0.1,
    epochs=20,
    batch_size=32
)

loss, acc = model.evaluate(
    x={
        'drug_1': X_test['drug_1_rxnorn_id'],
        'drug_2': X_test['drug_2_rxnorm_id'],
        'condition': X_test['condition_meddra_id'],
        'numeric': X_test[['A', 'B', 'C', 'D', 'PRR_error', 'mean_reporting_frequency']]
    },
    y=y_test
)

print(f'Test Accuracy: {acc:.2f}')

from sklearn.metrics import classification_report, confusion_matrix

y_pred = model.predict({
    'drug_1': X_test['drug_1_rxnorn_id'],
    'drug_2': X_test['drug_2_rxnorm_id'],
    'condition': X_test['condition_meddra_id'],
    'numeric': X_test[['A', 'B', 'C', 'D', 'PRR_error', 'mean_reporting_frequency']]
})
y_pred_binary = (y_pred > 0.5).astype(int)

print(confusion_matrix(y_test, y_pred_binary))
print(classification_report(y_test, y_pred_binary))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Load dataset
df = pd.read_csv('TWOSIDES.csv')  # Replace with actual path

# Label encoding
label_encoders = {}
for col in ['drug_1_rxnorn_id', 'drug_2_rxnorm_id', 'condition_meddra_id']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

# Features and label
X = df[['drug_1_rxnorn_id', 'drug_2_rxnorm_id', 'condition_meddra_id', 'A', 'B', 'C', 'D', 'PRR_error', 'mean_reporting_frequency']]
y = (df['PRR'] > 1.0).astype(int)  # Binary label

# Scale numeric features
scaler = StandardScaler()
X[['A', 'B', 'C', 'D', 'PRR_error', 'mean_reporting_frequency']] = scaler.fit_transform(
    X[['A', 'B', 'C', 'D', 'PRR_error', 'mean_reporting_frequency']]
)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vocabulary sizes
vocab_size = {
    'drug_1': df['drug_1_rxnorn_id'].nunique(),
    'drug_2': df['drug_2_rxnorm_id'].nunique(),
    'condition': df['condition_meddra_id'].nunique()
}

# Input layers
input_drug1 = Input(shape=(1,), name='drug_1')
input_drug2 = Input(shape=(1,), name='drug_2')
input_condition = Input(shape=(1,), name='condition')
input_numeric = Input(shape=(6,), name='numeric')

# Embeddings
embed_drug1 = Embedding(input_dim=vocab_size['drug_1'] + 1, output_dim=8)(input_drug1)
embed_drug2 = Embedding(input_dim=vocab_size['drug_2'] + 1, output_dim=8)(input_drug2)
embed_condition = Embedding(input_dim=vocab_size['condition'] + 1, output_dim=8)(input_condition)

# Flatten embeddings
flat_drug1 = Flatten()(embed_drug1)
flat_drug2 = Flatten()(embed_drug2)
flat_condition = Flatten()(embed_condition)

# Concatenate all
x = Concatenate()([flat_drug1, flat_drug2, flat_condition, input_numeric])

# Dense layers with Dropout and BatchNorm
x = Dense(128, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)

x = Dense(64, activation='relu')(x)
x = Dropout(0.3)(x)

x = Dense(32, activation='relu')(x)
x = Dropout(0.2)(x)

# Output layer
output = Dense(1, activation='sigmoid')(x)

# Build model
model = Model(inputs=[input_drug1, input_drug2, input_condition, input_numeric], outputs=output)
model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# Train model with early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)

history = model.fit(
    x={
        'drug_1': X_train['drug_1_rxnorn_id'],
        'drug_2': X_train['drug_2_rxnorm_id'],
        'condition': X_train['condition_meddra_id'],
        'numeric': X_train[['A', 'B', 'C', 'D', 'PRR_error', 'mean_reporting_frequency']]
    },
    y=y_train,
    validation_split=0.1,
    epochs=50,
    batch_size=64,
    callbacks=[early_stop]
)

# Evaluate
loss, acc = model.evaluate(
    x={
        'drug_1': X_test['drug_1_rxnorn_id'],
        'drug_2': X_test['drug_2_rxnorm_id'],
        'condition': X_test['condition_meddra_id'],
        'numeric': X_test[['A', 'B', 'C', 'D', 'PRR_error', 'mean_reporting_frequency']]
    },
    y=y_test
)

print(f'Test Accuracy: {acc:.4f}')

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Load data
df = pd.read_csv("TWOSIDES.csv")  # Replace with your file path

# Label encoding
label_encoders = {}
for col in ['drug_1_rxnorn_id', 'drug_2_rxnorm_id', 'condition_meddra_id']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

# Target label
y = (df['PRR'] > 1.0).astype(int)
df = df.drop(columns=['PRR'])  # Remove target leakage

# 👇 REMOVE all numeric features
X = df[['drug_1_rxnorn_id', 'drug_2_rxnorm_id', 'condition_meddra_id']]

# 👇 Custom drug-based train-test split (simulate unseen drugs)
unique_drugs = X['drug_1_rxnorn_id'].unique()
np.random.shuffle(unique_drugs)
split_idx = int(len(unique_drugs) * 0.8)
train_drugs = unique_drugs[:split_idx]
test_drugs = unique_drugs[split_idx:]

train_idx = X['drug_1_rxnorn_id'].isin(train_drugs)
test_idx = X['drug_1_rxnorn_id'].isin(test_drugs)

X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]


# Vocab sizes
vocab_size = {
    'drug_1': X['drug_1_rxnorn_id'].nunique(),
    'drug_2': X['drug_2_rxnorm_id'].nunique(),
    'condition': X['condition_meddra_id'].nunique()
}

# Inputs
input_drug1 = Input(shape=(1,), name='drug_1')
input_drug2 = Input(shape=(1,), name='drug_2')
input_condition = Input(shape=(1,), name='condition')

# Small embeddings
embed_drug1 = Embedding(input_dim=vocab_size['drug_1'] + 1, output_dim=4)(input_drug1)
embed_drug2 = Embedding(input_dim=vocab_size['drug_2'] + 1, output_dim=4)(input_drug2)
embed_condition = Embedding(input_dim=vocab_size['condition'] + 1, output_dim=4)(input_condition)

# Flatten
flat_drug1 = Flatten()(embed_drug1)
flat_drug2 = Flatten()(embed_drug2)
flat_condition = Flatten()(embed_condition)

# Concatenate
x = Concatenate()([flat_drug1, flat_drug2, flat_condition])

# Simple dense layer
x = Dense(16, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(1, activation='sigmoid')(x)

# Build model
model = Model(inputs=[input_drug1, input_drug2, input_condition], outputs=output)
model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001, restore_best_weights=True)

# Train
history = model.fit(
    x={
        'drug_1': X_train['drug_1_rxnorn_id'],
        'drug_2': X_train['drug_2_rxnorm_id'],
        'condition': X_train['condition_meddra_id']
    },
    y=y_train,
    validation_split=0.1,
    epochs=50,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

# Evaluate
loss, acc = model.evaluate(
    x={
        'drug_1': X_test['drug_1_rxnorn_id'],
        'drug_2': X_test['drug_2_rxnorm_id'],
        'condition': X_test['condition_meddra_id']
    },
    y=y_test
)

print(f"Test Accuracy: {acc:.4f}")

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Load data
df = pd.read_csv("TWOSIDES.csv")  # Replace with your file path

# Label encoding
label_encoders = {}
for col in ['drug_1_rxnorn_id', 'drug_2_rxnorm_id', 'condition_meddra_id']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

# Target label
y = (df['PRR'] > 1.0).astype(int)
df = df.drop(columns=['PRR'])  # Remove target leakage

# 👇 REMOVE all numeric features
X = df[['drug_1_rxnorn_id', 'drug_2_rxnorm_id', 'condition_meddra_id']]

# 👇 Custom drug-based train-test split (simulate unseen drugs)
unique_drugs = X['drug_1_rxnorn_id'].unique()
np.random.shuffle(unique_drugs)
split_idx = int(len(unique_drugs) * 0.8)
train_drugs = unique_drugs[:split_idx]
test_drugs = unique_drugs[split_idx:]

train_idx = X['drug_1_rxnorn_id'].isin(train_drugs)
test_idx = X['drug_1_rxnorn_id'].isin(test_drugs)

X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]


# Vocab sizes
vocab_size = {
    'drug_1': X['drug_1_rxnorn_id'].nunique(),
    'drug_2': X['drug_2_rxnorm_id'].nunique(),
    'condition': X['condition_meddra_id'].nunique()
}

# Inputs
input_drug1 = Input(shape=(1,), name='drug_1')
input_drug2 = Input(shape=(1,), name='drug_2')
input_condition = Input(shape=(1,), name='condition')

# Small embeddings
embed_drug1 = Embedding(input_dim=vocab_size['drug_1'] + 1, output_dim=4)(input_drug1)
embed_drug2 = Embedding(input_dim=vocab_size['drug_2'] + 1, output_dim=4)(input_drug2)
embed_condition = Embedding(input_dim=vocab_size['condition'] + 1, output_dim=4)(input_condition)

# Flatten
flat_drug1 = Flatten()(embed_drug1)
flat_drug2 = Flatten()(embed_drug2)
flat_condition = Flatten()(embed_condition)

# Concatenate
x = Concatenate()([flat_drug1, flat_drug2, flat_condition])

# Simple dense layer
x = Dense(16, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(1, activation='sigmoid')(x)

# Build model
model = Model(inputs=[input_drug1, input_drug2, input_condition], outputs=output)
model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.001, restore_best_weights=True)

# Train
history = model.fit(
    x={
        'drug_1': X_train['drug_1_rxnorn_id'],
        'drug_2': X_train['drug_2_rxnorm_id'],
        'condition': X_train['condition_meddra_id']
    },
    y=y_train,
    validation_split=0.1,
    epochs=50,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

# Evaluate
loss, acc = model.evaluate(
    x={
        'drug_1': X_test['drug_1_rxnorn_id'],
        'drug_2': X_test['drug_2_rxnorm_id'],
        'condition': X_test['condition_meddra_id']
    },
    y=y_test
)

print(f"Test Accuracy: {acc:.4f}")

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Load data
df = pd.read_csv("TWOSIDES.csv")  # Replace with your file path

# Label encoding
label_encoders = {}
for col in ['drug_1_rxnorn_id', 'drug_2_rxnorm_id', 'condition_meddra_id']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

# Target label
y = (df['PRR'] > 1.0).astype(int)
df = df.drop(columns=['PRR'])  # Remove target leakage

# REMOVE all numeric features
X = df[['drug_1_rxnorn_id', 'drug_2_rxnorm_id', 'condition_meddra_id']]

# Custom drug-based train-test split (simulate unseen drugs)
unique_drugs = X['drug_1_rxnorn_id'].unique()
np.random.shuffle(unique_drugs)
split_idx = int(len(unique_drugs) * 0.8)
train_drugs = unique_drugs[:split_idx]
test_drugs = unique_drugs[split_idx:]

train_idx = X['drug_1_rxnorn_id'].isin(train_drugs)
test_idx = X['drug_1_rxnorn_id'].isin(test_drugs)

X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]

# Vocab sizes
vocab_size = {
    'drug_1': X['drug_1_rxnorn_id'].nunique(),
    'drug_2': X['drug_2_rxnorm_id'].nunique(),
    'condition': X['condition_meddra_id'].nunique()
}

# Inputs
input_drug1 = Input(shape=(1,), name='drug_1')
input_drug2 = Input(shape=(1,), name='drug_2')
input_condition = Input(shape=(1,), name='condition')

# Small embeddings
embed_drug1 = Embedding(input_dim=vocab_size['drug_1'] + 1, output_dim=4)(input_drug1)
embed_drug2 = Embedding(input_dim=vocab_size['drug_2'] + 1, output_dim=4)(input_drug2)
embed_condition = Embedding(input_dim=vocab_size['condition'] + 1, output_dim=4)(input_condition)

# Flatten
flat_drug1 = Flatten()(embed_drug1)
flat_drug2 = Flatten()(embed_drug2)
flat_condition = Flatten()(embed_condition)

# Concatenate
x = Concatenate()([flat_drug1, flat_drug2, flat_condition])

# Simple dense layer
x = Dense(16, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(1, activation='sigmoid')(x)

# Build model
model = Model(inputs=[input_drug1, input_drug2, input_condition], outputs=output)
model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.001, restore_best_weights=True)

# Train
history = model.fit(
    x={
        'drug_1': X_train['drug_1_rxnorn_id'],
        'drug_2': X_train['drug_2_rxnorm_id'],
        'condition': X_train['condition_meddra_id']
    },
    y=y_train,
    validation_split=0.1,
    epochs=50,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

# Evaluate
loss, acc = model.evaluate(
    x={
        'drug_1': X_test['drug_1_rxnorn_id'],
        'drug_2': X_test['drug_2_rxnorm_id'],
        'condition': X_test['condition_meddra_id']
    },
    y=y_test
)

print(f"Test Accuracy: {acc:.4f}")

# Confusion Matrix
y_pred_prob = model.predict({
    'drug_1': X_test['drug_1_rxnorn_id'],
    'drug_2': X_test['drug_2_rxnorm_id'],
    'condition': X_test['condition_meddra_id']
})

y_pred = (y_pred_prob > 0.5).astype(int)

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["PRR ≤ 1", "PRR > 1"])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()